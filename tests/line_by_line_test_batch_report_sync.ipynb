{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line-by-line Test of the Batch Report Sync\n",
    "This is a notebook that tests the main script 2_batch_report_sync line by line with outputs for the purpose of debugging. The main script follows these steps:\n",
    "\n",
    "1. import installed packages and supporting modules\n",
    "2. set up directories and logging\n",
    "3. sync s3 bucket with stanford's sherlock oak folder\n",
    "4. read batch report and upload it to reglab's aws database\n",
    "5. save log file and sync to s3 bucket\n",
    "\n",
    "This notebook tests each of the code block and validates outputs.\n",
    "\n",
    "## Prerequisites for replication:\n",
    "1. must have Sherlock OAK and GROUP_SCRATCH mounted on your local machine, see guide.\n",
    "2. must have saved OAK and GROUP_SCRATCH as environment variables in your .bash_profile or .zshrc file. For example, \n",
    "```\n",
    "# sherlock directories\n",
    "export OAK=\"~/sherlock_oak\"\n",
    "export GROUP_SCRATCH=\"~/sherlock_group_scratch\"\n",
    "```\n",
    "3. must have the esnc_risk_notif git repo cloned to our local machine\n",
    "4. must have set up AWS web service and saved access key id and access key. See [this guide](https://realpython.com/python-boto3-aws-s3/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first set working directory as where the 0_email_maker will sit\n",
    "import os \n",
    "\n",
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import packages\n",
    "\n",
    "In this step, we are checking whether all the required modules have been installed in the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import installed packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import logging\n",
    "from io import StringIO\n",
    "\n",
    "## for s3 connection\n",
    "import boto3\n",
    "import subprocess\n",
    "\n",
    "# import supporting modules\n",
    "import configs\n",
    "from utilities import sql_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parsed arguments\n",
    "mode = 'test'\n",
    "run_id = '2021Q4_2021-08-03_170618_610692'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: set up directories and logging\n",
    "\n",
    "In this step, we are configuring directories and logging file. We should expect to see global variables from `configs` read correctly and the logging file prints out relevant lines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(configs.HELPER_TEXT_BATCH_REPORT_SYNC)\n",
    "print(\"===== Start running batch report sync =====\")\n",
    "\n",
    "# ## get parsed variables\n",
    "# args = get_args()\n",
    "# mode = args.mode\n",
    "# run_id = args.run_id\n",
    "\n",
    "## get global variables\n",
    "engine = configs.ENGINE\n",
    "bucket = configs.BUCKET\n",
    "s3_project_dir = configs.S3_PROJECT_DIR\n",
    "oak_project_dir = configs.OAK_PROJECT_DIR\n",
    "\n",
    "## set directories\n",
    "s3_run_dir = os.path.join(mode, run_id)\n",
    "oak_run_dir = os.path.join(oak_project_dir, mode, run_id)\n",
    "oak_log_dir = os.path.join(oak_run_dir, 'logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## configure logging\n",
    "logger, log_capture_string = configs.configure_logging(logger_name = 'batch_report_sync')\n",
    "logger.info(configs.HELPER_TEXT_BATCH_REPORT_SYNC)\n",
    "logger.info(\"Configured logger\")\n",
    "logger.info(f\"Log file to be saved in {oak_log_dir}\")\n",
    "logger.info(\"----- Parsed variables: mode = {}, run_id = {}\".format(mode, run_id))\n",
    "logger.info(\"----- S3 bucket: s3_project_dir = {}, s3_run_dir = {}\".format(s3_project_dir, s3_run_dir))\n",
    "logger.info(\"----- Sherlock OAK folders: oak_run_dir = {}\".format(oak_run_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print out variables and let the user confirm if they are correct and wish to proceed. \n",
    "print(\"----- Parsed variables: mode = {}, run_id = {}\".format(mode, run_id))\n",
    "print(\"----- S3 bucket: s3_project_dir = {}, s3_run_dir = {}\".format(s3_project_dir, s3_run_dir))\n",
    "print(\"----- Sherlock OAK folders: oak_run_dir = {}\".format(oak_run_dir))\n",
    "\n",
    "proceed = input('Please verify the above variables. Do you wish to proceed with the run? [y/n]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_capture_string.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: sync s3 bucket with stanford's sherlock oak folder\n",
    "\n",
    "In this step, we sync the S3 bucket to Sherlock Oak project folder. We should expect to retrieve batch reports and whippet sender log file from the oak folder. One quick way to check this would be to compare the whippet_sender log file in the s3 bucket and sherlock folder. They should be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"========= 1/3 Sync s3 bucket with Stanford's Sherlock OAK folder ==========\")\n",
    "subprocess.run(['aws', 's3', 'sync', s3_project_dir, oak_project_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the whippet_sender log file in s3 bucket and sherlock folder. they should be the sames\n",
    "s3_log_dir = os.path.join(mode, run_id, 'logs')\n",
    "s3_content = bucket.Object(os.path.join(s3_log_dir,'whippet_sender.log')).get()['Body'].read().decode('utf-8')\n",
    "with open(os.path.join(oak_log_dir, 'whippet_sender.log'), 'r') as file:\n",
    "    oak_content = file.read()\n",
    "s3_content == oak_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: upload batch report to reglab's aws database\n",
    "\n",
    "In this step, we upload batch report to RegLab's AWS database. We should expect to read the table from the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"======== 2/3 read batch report and upload it to reglab's aws database ========\")\n",
    "batch_report = pd.read_csv(os.path.join(oak_run_dir, 'batch_report.csv'))\n",
    "batch_report['run_id'] = run_id\n",
    "batch_report['file_timestamp'] = dt.datetime.now()\n",
    "sql_save.save_batch_report(mode = mode, batch_report = batch_report, engine = engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'prod':\n",
    "    with engine.begin() as conn:\n",
    "        df = pd.read_sql(\"SELECT * FROM esnc_risk_notif.batch_report\", conn)\n",
    "elif mode == 'test':\n",
    "    with engine.begin() as conn:\n",
    "        df = pd.read_sql(\"SELECT * FROM sandbox.esnc_notif_batch_report\", conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the table from the database is equal to the batch report from the folder\n",
    "df.equals(batch_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: save log file and sync to s3 bucket\n",
    "\n",
    "In this save, we save log file to Sherlock Oak project folder and sync the file to s3 bucket. We should expect the log file in the sherlock folder and s3 bucket to be the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('======= 3/3 save log file and sync to s3 bucket =======')\n",
    "logger.info(f'Script FINISHED. Log file saved in {oak_log_dir} and synced with S3 bucket.')\n",
    "with open(os.path.join(oak_log_dir, 'batch_report_sync.log'), 'w') as file:\n",
    "    file.write(log_capture_string.getvalue())\n",
    "\n",
    "subprocess.run(['aws', 's3', 'sync', oak_project_dir, s3_project_dir], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### validate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the log file in s3 bucket and sherlock folder. they should be the same\n",
    "file_name = 'batch_report_sync.log'\n",
    "s3_log_dir = os.path.join(mode, run_id, 'logs')\n",
    "s3_content = bucket.Object(os.path.join(s3_log_dir, file_name)).get()['Body'].read().decode('utf-8')\n",
    "\n",
    "with open(os.path.join(oak_log_dir, file_name), 'r') as file:\n",
    "    oak_content = file.read()\n",
    "\n",
    "s3_content == oak_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(oak_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "End of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
